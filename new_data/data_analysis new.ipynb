{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,mean_absolute_percentage_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set\n",
    "X_train = pd.read_csv('Data_n_model/X_train.csv')\n",
    "y_train = pd.read_csv('Data_n_model/y_train.csv')\n",
    "\n",
    "# Load testing set\n",
    "X_test = pd.read_csv('Data_n_model/X_test.csv')\n",
    "y_test = pd.read_csv('Data_n_model/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def print_metrics(model_name, grid=None):\n",
    "    train_pred = model_name.predict(X_train)\n",
    "    test_pred = model_name.predict(X_test)\n",
    "    print(f'Training Set\\nRMSE: {np.sqrt(mean_squared_error(y_train, train_pred))}')\n",
    "    print(f'MAE: {mean_absolute_error(y_train, train_pred)}')\n",
    "    print(f'MAPE: {mean_absolute_percentage_error(y_train, train_pred)}')\n",
    "    print(f'R2: {r2_score(y_train, train_pred)}\\n\\nTesting Set')\n",
    "    print(f'RMSE: {np.sqrt(mean_squared_error(y_test, test_pred))}')\n",
    "    print(f'MAE: {mean_absolute_error(y_test, test_pred)}')\n",
    "    print(f'MAPE: {mean_absolute_percentage_error(y_test, test_pred)}')\n",
    "    print(f'R2: {r2_score(y_test, test_pred)}')\n",
    "    if grid:\n",
    "        print(f'\\n\\nBest Estimator: {grid.best_estimator_}')\n",
    "        print(f'Best Parameters: {grid.best_params_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "# Save the tuned model\n",
    "joblib.dump(lr, 'r.pkl')\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_slr = lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation and Feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "RMSE: 90.79211205697973\n",
      "MAE: 50.89764255574441\n",
      "MAPE: 0.5843002561141697\n",
      "R2: 0.2050113097803734\n",
      "\n",
      "Testing Set\n",
      "RMSE: 93.33888468844106\n",
      "MAE: 52.69570060021923\n",
      "MAPE: 0.5875019384299192\n",
      "R2: 0.20296438308110387\n"
     ]
    }
   ],
   "source": [
    "print_metrics(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RandomForest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_srf = rf.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 7\n",
      "min_resources_: 20\n",
      "max_resources_: 14801\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 12\n",
      "n_resources: 20\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 4\n",
      "n_resources: 60\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 180\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_rf.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'max_depth': [None, 10, 20],\n",
    "}\n",
    "rf_grid = HalvingRandomSearchCV(rf, rf_param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "best_rf = rf_grid.best_estimator_\n",
    "joblib.dump(best_rf, 'best_rf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "RMSE: 64.93426631469589\n",
      "MAE: 34.564103087842874\n",
      "MAPE: 0.3780695315297801\n",
      "R2: 0.5933576673789547\n",
      "\n",
      "Testing Set\n",
      "RMSE: 80.71557183420876\n",
      "MAE: 41.351539363275265\n",
      "MAPE: 0.4240007320258586\n",
      "R2: 0.4039713026420939\n",
      "\n",
      "\n",
      "Best Estimator: RandomForestRegressor(max_depth=10, n_estimators=1000, random_state=42)\n",
      "Best Parameters: {'n_estimators': 1000, 'min_samples_split': 2, 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "print_metrics(best_rf, rf_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Regressor\n",
    "gbm = GradientBoostingRegressor(random_state=1)\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_sgbm = gbm.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 7\n",
      "min_resources_: 20\n",
      "max_resources_: 14801\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 8\n",
      "n_resources: 20\n",
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 3\n",
      "n_resources: 60\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "\n",
      "Testing Metrics - Hyper-tuned model:\n",
      "Training Set\n",
      "RMSE: 89.82012984294454\n",
      "MAE: 48.3058250713096\n",
      "MAPE: 0.5504144012980455\n",
      "R2: 0.22194182652784344\n",
      "\n",
      "Testing Set\n",
      "RMSE: 93.06992007902366\n",
      "MAE: 50.22039974565596\n",
      "MAPE: 0.5501204069603829\n",
      "R2: 0.20755122819589866\n",
      "\n",
      "\n",
      "Best Estimator: GradientBoostingRegressor(learning_rate=0.01, n_estimators=200, random_state=1)\n",
      "Best Parameters: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for Gradient Boosting\n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "gbm_grid = HalvingRandomSearchCV(gbm, gbm_param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)\n",
    "gbm_grid.fit(X_train, y_train)\n",
    "best_gbm = gbm_grid.best_estimator_\n",
    "joblib.dump(best_gbm, 'best_gbm.pkl')\n",
    "\n",
    "print(\"\\nTesting Metrics - Hyper-tuned model:\")\n",
    "print_metrics(best_gbm, gbm_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Classification\n",
    "svm = SVR(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predcition\n",
    "y_pred_train_ssvm = svm.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for SVM\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "}\n",
    "svm_grid = RandomizedSearchCV(svm, svm_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)\n",
    "svm_grid.fit(X_train, y_train)\n",
    "best_svm = svm_grid.best_estimator_\n",
    "joblib.dump(best_svm, 'best_svm.pkl')\n",
    "\n",
    "print_metrics(best_svm, svm_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Metrics - Standard model:\")\n",
    "print_metrics(y_train, y_pred_train_ssvm)\n",
    "\n",
    "print(\"\\nTraining Metrics - Hyper-tuned model:\")\n",
    "print_metrics(y_train, y_pred_train_bsvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logistic Model: Dependent is continuous so not applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Logistic Regression\n",
    "logit = LogisticRegression(max_iter=1000, random_state=1)\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train_slogit = logit.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Decision Tree Regressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(random_state=1)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_sdt = dt.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_param_grid = {\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "dt_grid = RandomizedSearchCV(dt, dt_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)\n",
    "dt_grid.fit(X_train, y_train)\n",
    "best_dt = dt_grid.best_estimator_\n",
    "joblib.dump(best_dt, 'best_dt.pkl')\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_bdt = best_dt.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print metrics\n",
    "print(\"Training Metrics - Standard model:\")\n",
    "print_metrics(y_train, y_pred_train_sdt)\n",
    "\n",
    "print(\"\\nTraining Metrics - Hyper-tuned model:\")\n",
    "print_metrics(y_train, y_pred_train_bdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances: Standard model\n",
    "importances = dt.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = X_train.columns  # Assuming df has columns attribute\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Feature importances: Standard model\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importances_standard_dt.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot feature importances: Hyper-tuned model\n",
    "importances = best_dt.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Feature importances: Hyper-tuned model\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importances_hyper_dt.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extra Trees Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Extra Trees Regressor\n",
    "et = ExtraTreesRegressor(random_state=1)\n",
    "et.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_set = et.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "et_grid = RandomizedSearchCV(et, et_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)\n",
    "et_grid.fit(X_train, y_train)\n",
    "best_et = et_grid.best_estimator_\n",
    "joblib.dump(best_et, 'best_et.pkl')\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_bet = best_et.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Metrics\n",
    "print(\"Training Metrics - Standard model:\")\n",
    "print_metrics(y_train, y_pred_train_set)\n",
    "\n",
    "print(\"\\nTraining Metrics - Hyper-tuned model:\")\n",
    "print_metrics(y_train, y_pred_train_bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances: Standard model\n",
    "importances = et.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = X_train.columns  # Assuming df has columns attribute\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Feature importances: Standard model\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importances_standard_et.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot feature importances: Hyper-tuned model\n",
    "importances = best_et.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Feature importances: Hyper-tuned model\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importances_hyper_et.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPRegressor(random_state=1)\n",
    "nn.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_snn = nn.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "nn_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam']\n",
    "}\n",
    "nn_grid = RandomizedSearchCV(nn, nn_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)\n",
    "nn_grid.fit(X_train, y_train)\n",
    "best_nn = nn_grid.best_estimator_\n",
    "joblib.dump(best_nn, 'best_nn.pkl')\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_bnn = best_nn.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Training Metrics\n",
    "print(\"Training Metrics - Standard model:\")\n",
    "print_metrics(y_train, y_pred_train_snn)\n",
    "\n",
    "print(\"\\nTraining Metrics - Hyper-tuned model:\")\n",
    "print_metrics(y_train, y_pred_train_bnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Extra Trees Regressor\n",
    "y_train_xgb = pd.cut(y_train, bins=10, labels=False)\n",
    "\n",
    "xgb = XGBClassifier(objective = 'binary:logistic', seed = 1, nthread=4, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train, y_train_xgb)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_sxgb = xgb.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "xgb_grid = RandomizedSearchCV(xgb, xgb_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)\n",
    "xgb_grid.fit(X_train, y_train_xgb)\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "joblib.dump(best_xgb, 'best_xgb.pkl')\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_bxgb = best_et.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Metrics\n",
    "print(\"Training Metrics - Standard model:\")\n",
    "print_metrics(y_train_xgb, y_pred_train_sxgb)\n",
    "\n",
    "print(\"\\nTraining Metrics - Hyper-tuned model:\")\n",
    "print_metrics(y_train_xgb, y_pred_train_bxgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances: Standard model\n",
    "importances = xgb.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = X_train.columns  # Assuming df has columns attribute\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Feature importances: Standard model\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importances_standard_et.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot feature importances: Hyper-tuned model\n",
    "importances = best_xgb.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Feature importances: Hyper-tuned model\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importances_hyper_et.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
