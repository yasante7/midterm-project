{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CSig4enGCTXQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, HalvingGridSearchCV, HalvingRandomSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error,mean_absolute_percentage_error, r2_score\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import warnings\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zoOCCJZCTXY"
      },
      "source": [
        "### 3.1 Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH3wydG6CTXZ"
      },
      "source": [
        "# Models Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BwGxtfg4CTXa"
      },
      "outputs": [],
      "source": [
        "# Load training set\n",
        "X_train = pd.read_csv('Data_n_model/X_train.csv')\n",
        "y_train = pd.read_csv('Data_n_model/y_train.csv')\n",
        "\n",
        "# Load testing set\n",
        "X_test = pd.read_csv('Data_n_model/X_test.csv')\n",
        "y_test = pd.read_csv('Data_n_model/y_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JRF4sExaCTXa"
      },
      "outputs": [],
      "source": [
        "# Metrics\n",
        "def print_metrics(model_name, grid=None):\n",
        "    train_pred = model_name.predict(X_train)\n",
        "    test_pred = model_name.predict(X_test)\n",
        "    print(f'Training Set\\nRMSE: {np.sqrt(mean_squared_error(y_train, train_pred))}')\n",
        "    print(f'MAE: {mean_absolute_error(y_train, train_pred)}')\n",
        "    print(f'MAPE: {mean_absolute_percentage_error(y_train, train_pred)}')\n",
        "    print(f'R2: {r2_score(y_train, train_pred)}\\n\\nTesting Set')\n",
        "    print(f'RMSE: {np.sqrt(mean_squared_error(y_test, test_pred))}')\n",
        "    print(f'MAE: {mean_absolute_error(y_test, test_pred)}')\n",
        "    print(f'MAPE: {mean_absolute_percentage_error(y_test, test_pred)}')\n",
        "    print(f'R2: {r2_score(y_test, test_pred)}')\n",
        "    if grid:\n",
        "        print(f'\\n\\nBest Estimator: {grid.best_estimator_}')\n",
        "        print(f'Best Parameters: {grid.best_params_}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5a6A9IICTXb"
      },
      "source": [
        "## 1. Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCFwW5z2CTXb"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KBu8se2bCTXc"
      },
      "outputs": [],
      "source": [
        "# Linear regression model\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "# Save the tuned model\n",
        "joblib.dump(lr, 'lr.pkl')\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_slr = lr.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ1v7pYHCTXc"
      },
      "source": [
        "### Model evaluation and Feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEfHzm7DCTXc",
        "outputId": "65ffa10b-b053-4813-ad40-f36951644857"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Metrics - Standard model:\n",
            "RMSE: 90.79211205697973\n",
            "MAE: 50.89764255574441\n",
            "MAPE: 0.5843002561141697\n",
            "R2: 0.2050113097803734\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Metrics - Standard model:\")\n",
        "print_metrics(y_train, y_pred_train_slr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6lRls7WCTXd"
      },
      "source": [
        "## 2. RandomForest Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX9trNd7CTXd"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HM-mGNzKCTXd"
      },
      "outputs": [],
      "source": [
        "# Linear regression model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_srf = rf.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPFgrl_6CTXe",
        "outputId": "5e728c42-f189-4ada-b029-eb01bb6d4759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_iterations: 3\n",
            "n_required_iterations: 3\n",
            "n_possible_iterations: 8\n",
            "min_resources_: 6\n",
            "max_resources_: 14801\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 12\n",
            "n_resources: 6\n",
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 4\n",
            "n_resources: 18\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 2\n",
            "n_resources: 54\n",
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter tuning for Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "rf_grid = HalvingRandomSearchCV(rf, rf_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=10)\n",
        "rf_grid.fit(X_train, y_train)\n",
        "best_rf = rf_grid.best_estimator_\n",
        "joblib.dump(best_rf, 'best_rf.pkl')\n",
        "\n",
        "y_pred_train_brf = best_rf.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ4W5NPNCTXe"
      },
      "source": [
        "### Model evaluation and Feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pbK375p2CTXe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Set\n",
            "RMSE: 64.99988156393702\n",
            "MAE: 34.5566536311386\n",
            "MAPE: 0.37707997314128777\n",
            "R2: 0.5925354383648671\n",
            "\n",
            "Testing Set\n",
            "RMSE: 80.94707771974171\n",
            "MAE: 41.438800160145036\n",
            "MAPE: 0.42385787819466475\n",
            "R2: 0.40054737763817305\n",
            "\n",
            "\n",
            "Best Estimator: RandomForestRegressor(max_depth=10, random_state=42)\n",
            "Best Parameters: {'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 10}\n"
          ]
        }
      ],
      "source": [
        "print_metrics(best_rf, rf_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BLXlbpsCTXf"
      },
      "source": [
        "## 3. Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfNxbxUUCTXf"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AyR8HzjICTXf"
      },
      "outputs": [],
      "source": [
        "# Gradient Boosting Regressor\n",
        "gbm = GradientBoostingRegressor(random_state=42)\n",
        "gbm.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_sgbm = gbm.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTNf2yv_CTXf"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5UJGGTTrCTXf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_iterations: 2\n",
            "n_required_iterations: 2\n",
            "n_possible_iterations: 8\n",
            "min_resources_: 6\n",
            "max_resources_: 14801\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 8\n",
            "n_resources: 6\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 3\n",
            "n_resources: 18\n",
            "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter tuning for Gradient Boosting\n",
        "gbm_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 5]\n",
        "}\n",
        "gbm_grid = HalvingRandomSearchCV(gbm, gbm_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=10, random_state=42)\n",
        "gbm_grid.fit(X_train, y_train)\n",
        "best_gbm = gbm_grid.best_estimator_\n",
        "joblib.dump(best_gbm, 'best_gbm.pkl')\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_bgbm = best_rf.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A426lUelCTXf"
      },
      "source": [
        "### Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZsYmGv9CTXf"
      },
      "outputs": [],
      "source": [
        "print_metrics(y_train, y_pred_train_bgbm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZVN6f17CTXg"
      },
      "outputs": [],
      "source": [
        "# Plot feature importances: Standard model\n",
        "importances = gbm.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title(\"Feature importances: Standard model\")\n",
        "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importances.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot feature importances: Hyper-tuned model\n",
        "importances = best_gbm.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title(\"Feature importances: Hyper-tuned model\")\n",
        "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importances.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMfdHnjmCTXg"
      },
      "source": [
        "## 4. Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjMa_EVICTXg"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC8Qdyz_CTXg"
      },
      "outputs": [],
      "source": [
        "# Support Vector Classification\n",
        "svm = SVC(kernel=\"rbf\" , probability = True, cache_size = 600)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predcition\n",
        "y_pred_train_ssvm = svm.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cunatliBCTXg"
      },
      "source": [
        "### Hyper-parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCg3Jie4CTXg"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for SVM\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'kernel': ['rbf', 'linear']\n",
        "}\n",
        "svm_grid = RandomizedSearchCV(svm, svm_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)\n",
        "svm_grid.fit(X_train, y_train)\n",
        "best_svm = svm_grid.best_estimator_\n",
        "joblib.dump(best_svm, 'best_svm.pkl')\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_bsvm = best_rf.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN1PXmiECTXh"
      },
      "source": [
        "### Feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVTjfKDdCTXh"
      },
      "outputs": [],
      "source": [
        "print(\"Training Metrics - Standard model:\")\n",
        "print_metrics(y_train, y_pred_train_ssvm)\n",
        "\n",
        "print(\"\\nTraining Metrics - Hyper-tuned model:\")\n",
        "print_metrics(y_train, y_pred_train_bsvm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8tONnOTCTXh"
      },
      "source": [
        "## 5. Logistic Model: Dependent is continuous so not applicable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRIS_MDGCTXh"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz3HH4APCTXh"
      },
      "source": [
        "### Training Logistic Regression\n",
        "logit = LogisticRegression(max_iter=1000, random_state=1)\n",
        "logit.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train_slogit = logit.predict(X_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOiOkwRoCTXi"
      },
      "source": [
        "## 6. Decision Tree Regressor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5C9h2K2CTXi"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhZBVVjICTXr"
      },
      "outputs": [],
      "source": [
        "dt = DecisionTreeRegressor(random_state=1)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_sdt = dt.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVmA1CIxCTXr"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mXVcbtUCTXr"
      },
      "outputs": [],
      "source": [
        "dt_param_grid = {\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "dt_grid = RandomizedSearchCV(dt, dt_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)\n",
        "dt_grid.fit(X_train, y_train)\n",
        "best_dt = dt_grid.best_estimator_\n",
        "joblib.dump(best_dt, 'best_dt.pkl')\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_bdt = best_dt.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjzvxt25CTXr"
      },
      "source": [
        "### Model evaluation and Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WiilyTSCTXr"
      },
      "outputs": [],
      "source": [
        "# print metrics\n",
        "print(\"Training Metrics - Standard model:\")\n",
        "print_metrics(y_train, y_pred_train_sdt)\n",
        "\n",
        "print(\"\\nTraining Metrics - Hyper-tuned model:\")\n",
        "print_metrics(y_train, y_pred_train_bdt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xurgaDV-CTXs"
      },
      "outputs": [],
      "source": [
        "# Plot feature importances: Standard model\n",
        "importances = dt.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "features = X_train.columns  # Assuming df has columns attribute\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title(\"Feature importances: Standard model\")\n",
        "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importances_standard_dt.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot feature importances: Hyper-tuned model\n",
        "importances = best_dt.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title(\"Feature importances: Hyper-tuned model\")\n",
        "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importances_hyper_dt.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd2zWAOmCTXs"
      },
      "source": [
        "## 7. Extra Trees Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DcxbV1cCTXs"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdoD1BtaCTXs"
      },
      "outputs": [],
      "source": [
        "# Training Extra Trees Regressor\n",
        "et = ExtraTreesRegressor(random_state=1)\n",
        "et.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_set = et.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8e2OsswCTXs"
      },
      "source": [
        "### Hyperparameter Tuning for Extra Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn5SAUYZCTXs"
      },
      "outputs": [],
      "source": [
        "et_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "et_grid = RandomizedSearchCV(et, et_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)\n",
        "et_grid.fit(X_train, y_train)\n",
        "best_et = et_grid.best_estimator_\n",
        "joblib.dump(best_et, 'best_et.pkl')\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_bet = best_et.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6GsTx_rCTXt"
      },
      "source": [
        "### Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36wI9JmGCTXt"
      },
      "outputs": [],
      "source": [
        "# Model Metrics\n",
        "print(\"Training Metrics - Standard model:\")\n",
        "print_metrics(y_train, y_pred_train_set)\n",
        "\n",
        "print(\"\\nTraining Metrics - Hyper-tuned model:\")\n",
        "print_metrics(y_train, y_pred_train_bet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0prYj1wmCTXt"
      },
      "outputs": [],
      "source": [
        "# Plot feature importances: Standard model\n",
        "importances = et.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "features = X_train.columns  # Assuming df has columns attribute\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title(\"Feature importances: Standard model\")\n",
        "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importances_standard_et.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot feature importances: Hyper-tuned model\n",
        "importances = best_et.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title(\"Feature importances: Hyper-tuned model\")\n",
        "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importances_hyper_et.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2owyuEcECTXt"
      },
      "source": [
        "## 8. Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e76nNo6CTXt"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58MZhfp5CTXu"
      },
      "outputs": [],
      "source": [
        "nn = MLPRegressor(random_state=1)\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_snn = nn.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-sA6tUPCTXu"
      },
      "source": [
        "### Hyperparameter Tuning for Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1uhgghnCTXu"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "nn_param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam']\n",
        "}\n",
        "nn_grid = RandomizedSearchCV(nn, nn_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)\n",
        "nn_grid.fit(X_train, y_train)\n",
        "best_nn = nn_grid.best_estimator_\n",
        "joblib.dump(best_nn, 'best_nn.pkl')\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_bnn = best_nn.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS0BG3b7CTXu"
      },
      "source": [
        "### Training Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD8SlimCCTXu"
      },
      "outputs": [],
      "source": [
        "# Print Training Metrics\n",
        "print(\"Training Metrics - Standard model:\")\n",
        "print_metrics(y_train, y_pred_train_snn)\n",
        "\n",
        "print(\"\\nTraining Metrics - Hyper-tuned model:\")\n",
        "print_metrics(y_train, y_pred_train_bnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkLZ5XVNCTXv"
      },
      "source": [
        "## 9. XGBOOST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pYNZlJXCTXv"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEV-qhsNCTXv"
      },
      "outputs": [],
      "source": [
        "# Training Extra Trees Regressor\n",
        "y_train_xgb = pd.cut(y_train, bins=10, labels=False)\n",
        "\n",
        "xgb = XGBClassifier(objective = 'binary:logistic', seed = 1, nthread=4, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train, y_train_xgb)\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_sxgb = xgb.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "priKJyFmCTXv"
      },
      "source": [
        "### Hyperparameter Tuning for XG Boost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZZfd2mcCTXv"
      },
      "outputs": [],
      "source": [
        "# Define the parameter grid\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "xgb_grid = RandomizedSearchCV(xgb, xgb_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)\n",
        "xgb_grid.fit(X_train, y_train_xgb)\n",
        "best_xgb = xgb_grid.best_estimator_\n",
        "joblib.dump(best_xgb, 'best_xgb.pkl')\n",
        "\n",
        "# Predictions\n",
        "y_pred_train_bxgb = best_et.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwvtfONdCTXw"
      },
      "source": [
        "### Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "367ma-f6CTXw"
      },
      "outputs": [],
      "source": [
        "# Model Metrics\n",
        "print(\"Training Metrics - Standard model:\")\n",
        "print_metrics(y_train_xgb, y_pred_train_sxgb)\n",
        "\n",
        "print(\"\\nTraining Metrics - Hyper-tuned model:\")\n",
        "print_metrics(y_train_xgb, y_pred_train_bxgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y-9mkhHCTXw"
      },
      "outputs": [],
      "source": [
        "# Plot feature importances: Standard model\n",
        "importances = xgb.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "features = X_train.columns  # Assuming df has columns attribute\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title(\"Feature importances: Standard model\")\n",
        "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importances_standard_et.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot feature importances: Hyper-tuned model\n",
        "importances = best_xgb.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title(\"Feature importances: Hyper-tuned model\")\n",
        "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importances_hyper_et.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHe7EdXWCTXw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeVg5LaiCTXw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
