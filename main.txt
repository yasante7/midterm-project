
Abstract
This study investigates the prediction of appliance energy consumption in residential buildings using various machine learning techniques. To achieve this, eight machine learning models—Multiple Linear Regression, Support Vector Regression, Random Forest, Decision Tree, Gradient Boosting, XGBoost, Extra Trees, and Neural Networks—were trained and evaluated using the "Appliances Energy Prediction" dataset from the UCI Machine Learning Repository. Extensive feature engineering and hyperparameter tuning were conducted to optimize model performance. The models were assessed using metrics such as Root Mean Square Error (RMSE), R-squared (R²), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE). The results indicated that ensemble models like Extra Trees and Random Forest provided superior RMSE and R-Squared in predicting energy consumption, outperforming other traditional models. The Extra Trees model achieved a testing RMSE of 63.07 and an R² of 0.60, while the Random Forest model had a testing RMSE of 65.11 and an R² of 0.57, significantly outperforming other traditional models.
Introduction
Energy consumption in residential buildings is a significant component of overall energy usage, contributing to both economic costs and environmental impacts. Therefore, accurate prediction of appliance energy use is crucial for developing effective energy management strategies and promoting energy efficiency. Building upon the work of previous researchers [1], [2], this project employs eight machine learning techniques to predict appliance energy use in residential buildings with the objective on determining the algorithm that best predicts this usage. The study is also significant as it enhances the understanding of how different machine learning models can predict energy consumption, providing insights into which algorithms are most effective in this context. The findings can inform the design of smarter, more energy-efficient residential buildings and contribute to the advancement of sustainable living practices.
The primary objective of this study is to predict appliance energy consumption focusing on model improvement. The study specifically seeks:
	To introduce new models – Decision Trees and Random Forest models – and find out if they can outperform the models machine models used in [1], [2].
	To introduce additional feature engineering in the UCI Appliance energy Prediction data and found out if they are important in influencing appliance energy consumption.
Despite its contributions, this study has several limitations. Firstly, the dataset is limited to a specific geographic location and time period, which may not generalize to other regions or conditions. The study also relies heavily on the quality and granularity of the data, meaning that any inaccuracies or missing data could impact the model's performance. Additionally, the machine learning models used in this study are influenced by the chosen hyperparameters and the extent of feature engineering, which might limit the reproducibility of the results. Moreover, while the study compares several models, it does not explore all possible machine learning algorithms or techniques, which might also impact the conclusions drawn. Finally, the study does not fully address the potential external factors like human behaviour variations or unexpected appliance use that could affect energy consumption predictions.
Literature Review
Factors Influencing Appliance Energy Use in Residential Buildings
Building characteristics and household compositions significantly influence residential energy consumption. Xie and Noor [3] conducted a comprehensive analysis using multiple regression models to investigate determinants of residential end-use energy consumption, focusing on building characteristics, household compositions, lifestyles, and home equipment. Their study identified floor area and the number of family members as key factors affecting energy use for various purposes such as cooling and appliances. Similarly, Rickwood [4] and Kavousian et al. [5] utilized detailed analyses controlling for household demographic and income variables to demonstrate that dwelling type, household size, and household income significantly influence energy consumption.
Another important determinant of residential energy consumption in the literature is appliance ownership and usage patterns. Leahy and Lyons [6] employed logit and OLS regression analyses on a large micro-dataset to show how household characteristics explain appliance ownership and its effect on energy demand. They noted that the type and number of appliances owned by a household directly impact overall energy use. Additionally, studies by Cetin et al. [7] and Kavousian et al. [8] analyzed disaggregated energy use data and energy efficiency frontiers to highlight that usage patterns, such as the time of day and frequency of use, significantly affect energy consumption. These studies found that user-dependent appliances, like washers and dryers, vary greatly in their energy use patterns, which can influence peak demand and overall energy consumption.
Behavioral factors of household occupants are also found to be important in influencing residential energy consumption. Rouleau et al. [9] conducted a case study of a high-performance Canadian social housing building, using regression analysis and monitoring systems to assess energy use and occupant behavior. They found that occupant behavior, such as the frequency of opening windows or the use of electrical appliances, significantly impacts energy use, often outweighing structural factors. Households with higher education levels and those that actively track their energy usage tend to be more energy-efficient, demonstrating the importance of awareness and proactive energy management. This was supported by Kavousian et al. [8] through their analysis of smart meter data and energy efficiency rankings.
Technological advancements and environmental conditions have also been found to play significant roles in residential energy consumption. Iwayemi et al. [10] discussed the potential of smart consumption strategies and energy management systems to enhance energy efficiency without compromising living standards. They highlighted the benefits of advanced technologies and energy-efficient appliances in reducing energy consumption. Reyna and Chester [11] conducted a study using climate change projections to assess residential electricity and natural gas demand in Los Angeles. They found that climate change could increase residential energy demand substantially, and therefore recommended the need for aggressive energy efficiency policies to mitigate this impact.
Use of AI models for predicting energy use
Candanedo et al. [1] and Assadian & Assadian [2] utilized various machine learning techniques, including gradient boosting machines (GBM), random forest, and extra trees regressor, for data-driven prediction of appliance energy use. These studies showed Gradient Boosting Machines (GBM) and Extra Trees Regressor having high accuracy in predicting appliance energy consumption. Both studies used root mean square error (RMSE), coefficient of determination (R2), mean absolute error (MAE), and mean absolute percentage error (MAPE) to evaluate model performance. Ullah et al [12] investigated the prediction of appliance energy consumption in low-energy homes using machine learning algorithms. They benchmarked eight algorithms including Linear, Ridge, and LASSO regression; Support Vector Machine; Multilayer Perceptron; Nearest Neighbor regression; Extra-Trees; and XG-Boost. The study evaluated these algorithms based on metrics such as R-squared, MAE, RMSE, and training time. Extra-Trees and XG-Boost emerged as the top-performing algorithms while demonstrating efficient performance across the specified error metrics. Duarte et al. [13] evaluated the performance of multiple machine learning techniques such as ANN, SVR, and Random Forests, concluding that ANN combined with interaction variables offered the best prediction accuracy. This study also used the mean absolute error (MAE), root mean square error (RMSE), coefficient of determination (R2), and mean absolute percentage error (MAPE) to evaluate the performance of the machine learning models. Rambabu et al. [14] emphasize feature engineering and the training of various models such as Linear Regression, Lasso Regression, Random Forest, Extra Tree Regressor, and XG Boost for household energy consumption prediction. Evaluation was conducted using R-squared to assess predictive accuracy based on time-series data. The study concluded that tree-based models offer superior performance in forecasting household energy consumption patterns influenced by factors like temperature, humidity, and time of day.
Edwards et al. [15] evaluated seven machine learning algorithms for predicting hourly residential energy consumption using 15-minute interval data. Among the methods tested, Least Squares Support Vector Machines (LS-SVM) outperformed other models, including Neural Networks. Jain et al. [16] applied Support Vector Regression (SVR) to forecast energy consumption in multi-family residential buildings. They found that the optimal monitoring granularity occurs at the floor level with hourly intervals. Long Short-Term Memory (LSTM) networks demonstrated superior performance with the highest R2 (0.97) and lowest RMSE (21.36) over traditional machine learning models in testing set [17].
Deep learning models, including Conditional Restricted Boltzmann Machine (CRBM) and Factored CRBM, outperformed traditional machine learning methods like SVM and ANN in time series prediction of energy consumption [18]. The study employed the root mean square error (RMSE) and the correlation coefficient as evaluation metrics. WaveNet models have been effective for energy disaggregation, outperforming other deep learning methods in terms of error measures and computational cost [19]. Bourhnane et al. [20] utilized Artificial Neural Networks (ANN) along with Genetic Algorithms to predict energy consumption in smart buildings. The ANN model demonstrated a modest prediction accuracy. Mocanu et al. Ngo et al. [21] proposed an ensemble machine learning model combining ANN, SVR, and M5Rules for predicting energy consumption in non-residential buildings. Their ensemble approach significantly improved prediction accuracy compared to individual models. Al-Rakhami et al. [22] developed an ensemble learning approach using extreme gradient boosting (XGBoost) to predict heating and cooling loads in residential buildings, achieving high prediction performance.
Nambiar et al. [23] explored the prediction of household appliance electric energy consumption using machine learning and deep learning algorithms such as Support Vector Regression (SVR), k-Nearest Neighbour (kNN), Decision Tree Regression (DTR), Fully Connected Neural Network, and Long Short-Term Memory (LSTM). Performance was assessed using Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) metrics to gauge prediction accuracy. Sajjad et al. [24] introduced a hybrid CNN-GRU model for short-term residential load forecasting. The models were assessed using RMSE, MAE and MAE as metrics. Experimental results demonstrated superior performance of the CNN-GRU model compared to traditional models like Gradient Boosting Regression (GBR), Artificial Neural Networks (ANNs), Extreme Learning Machine (ELM), and Support Vector Machine (SVM) on Appliances Energy Prediction (AEP) and Individual Household Electric Power Consumption (IHEPC) datasets.
Ibrahim et al. [25] used multiple linear regression (MLR) and multilayer perceptron (MLP) methods to estimate heating load (HL) and cooling load (CL) in residential buildings. Their study found that MLP provided accurate estimates with low MAE, RMSE, and high R2 values. Moradzadeh et al. [26] introduced a hybrid model combining Group Method of Data Handling (GMDH) and SVR to forecast CL and HL, which performed better than traditional regression models. The study employed the coefficient of determination (R2), mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE) to assess the performance of the models. Wu and He [27] conducted a comparative study on various machine learning models, including Gradient Boosting Regressor and HB-Regressor, for predicting heating and cooling loads. To determine the best performing model, the study employed the coefficient of determination (R2), mean absolute error (MAE), and mean squared error (MSE). The study also used the Accuracy and Error rate to access the overall performance. The study found that Gradient Boosting Regressor provided high prediction accuracy with efficient fitting speeds. [28] evaluated the effectiveness of different machine learning models for estimating energy consumption in smart homes. The study employed the root mean squared error (RMSE), the R – Squared (R2) and mean absolute error (MAE) to compare the models. The paper found the random forest model as the machine learning algorithm that accurately estimates energy consumption of appliances in smart homes.
Tran et al. [29] developed the Evolutionary Neural Machine Inference Model (ENMIM), which combines LSSVR and RBFNN with symbiotic organism search (SOS) for parameter optimization. This hybrid model demonstrated superior predictive accuracy compared to other benchmark models. Hybrid methods combining clustering (e.g., k-medoids) and machine learning models (e.g., SVM, ANN) have achieved high accuracy (99.2%) in forecasting appliance consumption and peak demand [30]. Zaini [31] discussed the use of a Feature Optimization Prediction Framework (FOPF) and KNN ensemble prediction models for predicting appliance energy usage in low-energy buildings. The study highlights the effectiveness of KNN models, which showed the best accuracy and lowest error (RMSE = 0.0078) among the tested machine learning algorithms.
Ahmed Al-Adaileh, S. Khaddaj [32] proposed an integrated smart energy management system that utilizes different machine learning regression techniques to predict and schedule the running periods of schedulable appliances in households. The system integrates data from the surrounding environment to enhance energy efficiency, achieving up to a 36% reduction in energy consumption. Iram et al. [33] proposed a decision algorithm model utilizing machine learning-based data mining and picture fuzzy operators. The study evaluated the performance of machine learning algorithms using accuracy metrics and employed a decision matrix with fuzzy operators to aggregate and rank the algorithms based on their predictive capabilities. The approach integrates Lasso Regression to analyze weather patterns and features affecting smart home micro-climates, providing insights into appliance electricity consumption and overall energy usage.
Optimization and Tuning
Hyperparameter Tuning
GridSearch systematically explores a specified range of hyperparameters by evaluating all possible combinations. For example, it has been used effectively in tuning models like Random Forests and Support Vector Machines [1], [2], [34], [35]. Although exhaustive, it can be computationally expensive [36]. Random Search randomly samples hyperparameters from a defined distribution, often more efficient than grid search in finding optimal configurations within a limited time [36].
Bayesian optimization has been applied successfully in optimizing hyperparameters for neural networks and gradient boosting models [37]. Similarly, evolutionary algorithms have been applied to tune models such as Random Forests and neural networks [38]. According to [39], evolutionary algorithms use mutation, crossover, and selection to optimize hyperparameters. Multi-objective optimization techniques, such as binary grey wolf optimization, have also been used to enhance the performance of models like Random Forest and K-Nearest Neighbor by optimizing feature selection [40].
Feature Selection and Engineering
Feature selection and engineering enhance model performance by focusing on the most relevant features and reducing dimensionality. The most used feature engineering techniques are the recursive feature elimination (RFE) and the principal component analysis (PCA). Recursive Feature Elimination (RFE) iteratively removes less important features based on model performance, helping improve efficiency and accuracy [41]. This technique is particularly useful for linear regression and support vector machines.
Principal Component Analysis (PCA), on the other hand, reduces dimensionality by transforming features into orthogonal components, capturing the most variance with fewer features [42]. PCA has been used effectively in logistic regression and neural network models to improve computational efficiency and accuracy [43].
Conclusion
The literature review provides several key insights that are more relevant for this study. Existing studies have highlighted the significant influence of building characteristics and household composition, appliance ownership and usage patterns, and environmental conditions on energy consumption. This informs the study of the relevance of including these variables or their proxies such as temperature and humidity across different rooms and the number of occupants as well as specific variables related to appliance usage, such as energy consumption by appliances and lights. External weather conditions, such as outdoor temperature, humidity, and wind speed are also included to represent the role of environmental factors on energy consumption.
The use of AI models for predicting energy consumption is another key area discussed in the literature. Various machine learning models, including Gradient Boosting Machines (GBM), Random Forest, Support Vector Regression (SVR), and Neural Networks (NN), are identified as effective tools for predicting energy consumption with high accuracy. This study employs a wide array of models, such as GBM, RF, SVR, and NN, to predict energy consumption, which is consistent with the literature’s recommendations on the effectiveness of these models.
The literature emphasizes the importance of hyperparameter tuning and feature selection for optimizing model performance. Techniques such as GridSearchCV, Bayesian optimization, and feature selection methods like Recursive Feature Elimination (RFE) are highlighted as critical for enhancing model accuracy and efficiency. This study implements HalvingGridSearchCV for efficient hyperparameter tuning and employs feature engineering to refine the input variables, directly informed by these best practices from the literature. Existing studies have also identified key metrics such as RMSE, MAE, R2, and MAPE for assessing model accuracy particularly in this area of study. 
Methodology
	Introduction
This chapter details the methodologies and techniques used in the project to predict appliance energy use in residential buildings. The chapter covers the types and sources of data, a brief description of the variables involved, the estimation techniques and evaluation metrics employed, the technologies used, data preprocessing steps, and the modeling procedures.
	Data Type and Source
The dataset used in this study is sourced from the UCI Machine Learning Repository [44], specifically the "Appliances Energy Prediction" dataset. This dataset contains 19,735 instances of measurements related to energy consumption in residential buildings, collected over a period. The data includes various features such as temperature, humidity, weather conditions, and electrical energy usage of appliances.
	Brief Description of Variables and Expected Signs
The dataset utilized in this research offers detailed information on appliance energy use in a residential environment. While the original paper thoroughly outlines the data collection methods and variables, this study provides a concise overview of the variables in Table 3.1. Various factors, such as temperature, humidity, and the energy consumption of different appliances, were tracked within a single-family home. Data were collected every 10 minutes using an internet-connected energy monitoring system and a wireless sensor network distributed across different rooms. This study aims to predict total appliance energy use, recorded in watt-hours (Wh), using these variables. Papers [2] and [1] give a comprehensive account of the data collection methods.
Table 3.1: Variables and their Description
Variable	Description
date	Time (year-month-day hour:minute)
Appliances	Energy use in Wh
lights	Energy use of light fixtures in the house in Wh
T1	Temperature in kitchen area, in Celsius
RH_1	Humidity in kitchen area, in %
T2	Temperature in living room area, in Celsius
RH_2	Humidity in living room area, in %
T3	Temperature in laundry room area, in Celsius
RH_3	Humidity in laundry room area, in %
T4	Temperature in office room, in Celsius
RH_4	Humidity in office room, in %
T5	Temperature in bathroom, in Celsius
RH_5	Humidity in bathroom, in %
T6	Temperature outside the building (north side), in Celsius
RH_6	Humidity outside the building (north side), in %
T7	Temperature in ironing room, in Celsius
RH_7	Humidity in ironing room, in %
T8	Temperature in teenager room 2, in Celsius
RH_8	Humidity in teenager room 2, in %
T9	Temperature in parents room, in Celsius
RH_9	Humidity in parents room, in %
To	Temperature outside (from Chièvres weather station), in Celsius
Pressure	Pressure (from Chièvres weather station), in mm Hg
RH_out	Humidity outside (from Chièvres weather station), in %
Windspeed	Windspeed (from Chièvres weather station), in m/s
Visibility	Visibility (from Chièvres weather station), in km
Tdewpoint	Tdewpoint (from Chièvres weather station), °C
rv1	Random variable 1, nondimensional
rv2	Random variable 2, nondimensional
	Data Preprocessing
Initial exploratory data analysis (EDA) was conducted to understand the data distribution and relationships between variables and also to check for data validity. Correlation analysis was conducted to help identify highly correlated variables, which was useful for feature selection and engineering.
In addition to the variables derived from the original data, which included the number of seconds from midnight (NSM), the classification of days as weekends or weekdays, and specific days of the week, further feature engineering was implemented using the timestamp data provided and following the outline by [2]. This approach aimed to extract additional insights from the temporal information that was not initially integrated into the modeling process. New features created included hour, month, and day features. This was extracted from the timestamp to capture temporal variations in energy consumption patterns. Another feature is the seasonal categorical data which was created based on the month, categorizing data into autumn, winter, spring, or summer. 
	Modelling
The study trained and evaluated eight models to predict appliance energy consumption: (a) multiple linear regression (LM), (b) support vector regression (SVR), (c) random forest (RF), (d) decision tree (DT), (e) gradient boosting (GB), (f) XGBoost (XGB), (g) the extra trees model (ET), and (h) neural network (NN).
Multiple Linear Regression (LM) establishes the relationship between the dependent variable (appliance energy use) and one or more independent variables by fitting a linear equation to the observed data. Support Vector Regression (SVR) utilizes support vectors to transform the input space into a higher-dimensional feature space where linear regression is performed. SVR aims to find a hyperplane that optimizes the separation between predicted and actual values, providing the best-fit line with the most points.
Random Forest (RF) is an ensemble learning method that builds multiple decision trees using a random subset of features. The best split is chosen from the subset based on information gain. Each tree uses a unique subset of data and variables, minimizing overfitting risks. The final prediction is made by averaging all decision trees' predictions. Decision Tree (DT) involves recursively partitioning the data into subsets based on input feature values until a stopping criterion, such as maximum tree depth, is met.
Gradient Boosting (GB) constructs a sequence of decision trees, with each subsequent tree trained to correct the errors of the previous one. The algorithm minimizes a loss function, like mean squared error (MSE), by iteratively adding decision trees to the ensemble until the loss is satisfactorily minimized. XGBoost (XGB), similar to gradient boosting, incorporates several regularization techniques, including L1/L2 regularization, tree pruning, and early stopping, to prevent overfitting and enhance model generalization. XGBoost also supports parallel tree boosting.
Extra Trees (ET) are similar to random forests but select splits randomly without considering split quality. This method accelerates the training process and increases tree diversity, improving model generalization. Neural Network (NN) models consist of interconnected layers of nodes (neurons), each connection having an associated weight. During training, these weights are adjusted to minimize prediction errors. Neural networks can model complex, non-linear relationships in the data.
	Training and Testing Procedure
The dataset was split into training and testing sets to evaluate model performance. All eight regression models were trained using 10-fold cross-validation to ensure robust evaluation of the models. This method divides the dataset into 10 subsets, and each model is trained and validated 10 times, with a different subset serving as the validation set for each iteration [2], [45]. The final performance is averaged across all 3 iterations, providing a consistent estimate of the model's performance by utilizing diverse data rather than a single randomized split.
Hyperparameter tuning was performed using HalvingGridSearchCV. Unlike a standard grid search, where every combination is evaluated exhaustively, HalvingGridSearchCV is more efficient. It starts by testing all the hyperparameter combinations on a small subset of the data, then it continuously narrows down the pool of combinations by eliminating the worst-performing ones while increasing the amount of data used in each step. This halving process continues until only the best-performing combinations remain, which are then evaluated more thoroughly. This approach reduces the computational cost and provides a more efficient path to finding the optimal hyperparameters for the model. In addition to the hyperparameters, the study employed the Bootstrapping approach to visualize a histogram of how the Root of Mean Squared Error (RMSE) changes in different subsets of the data using the best performing model. The shape of this distribution reveals the model's sensitivity to changes in the data. A narrow distribution, where most RMSE values are clustered around a central value, indicates that the model is stable and its performance is consistent across different samples. Conversely, a wider distribution suggests that the model's performance may vary depending on the specific characteristics of the data it is trained on. This variability could highlight areas where the model is less robust, potentially guiding further refinements to improve its generalizability. The bootstrapping approach, therefore, provides a better understanding of model reliability, complementing traditional cross-validation techniques and offering greater confidence in the model's ability to perform well in diverse real-world scenarios.
For Support Vector Regression (SVR), the critical hyperparameters gamma, cost and kernel were tuned. The kernel type (rbf or linear) determines the function used to transform the data into a higher-dimensional space where it becomes easier to separate the classes. Gamma influences the shape of the decision boundary, while cost balances the trade-off between training error and testing error. Optimal values for SVR were found to be 1 for gamma 1 for cost and the best kernel as linear. Random Forest (RF) model required tuning of the number of trees (estimators) and the number of features considered for splits (max features). The optimal configuration for Random Forest model was determined to be 500 estimators and a max depth of None. In addition to the number of estimators and the max depth parameters, the minimum samples split was also tuned for the Extra Trees (ET) model. The optimal number of estimators were found to be 100, minimum samples split of 5 and maximum depth of 20 for ET model.
Similar to the Random Forest model, the Gradient Boosting (GB) tuned the number of estimators and the max_depth parameters. The optimal values were found to be 500 and 2, respectively.
Linear Regression (LM) does not typically require hyperparameter tuning but was included as a baseline model for comparison. The model fits a linear equation to the observed data, using all available features.
The additional models added to the work of [2] are the Decision Tree and the Neural Network models. For the Decision Tree model, hyperparameters were carefully tuned to optimize performance while avoiding overfitting. The optimal configuration was found to be a maximum depth of 20, with a criterion of 'absolute_error', a minimum of 4 samples per leaf, and a minimum of 2 samples per split. For the Neural Network model, several key hyperparameters were adjusted to achieve the best performance. The optimal parameters included an activation function of 'logistic', an alpha value of 0.0001 for regularization, and a single hidden layer with 100 neurons. Additionally, the learning rate was set to 'adaptive', with an initial learning rate of 0.01. These settings were selected to ensure the neural network could effectively learn from the data without overfitting, while also adapting the learning rate as training progressed.
	Model Evaluation
To assess and compare the performance of each model in predicting appliance energy consumption, several metrics were employed:
	Root Mean Square Error (RMSE): RMSE measures the average magnitude of the error between predicted and actual values, providing a sense of how well the model fits the data. A lower RMSE indicates better model performance, as it signifies smaller deviations between predicted and actual values. RMSE is defined as below:
RMSE= √((∑_(i=1)^n▒(Y-Y ̂ )^2 )/n)
	Coefficient of Determination (R-squared, R2): R-squared represents the proportion of the variance in the dependent variable (Y) that is predictable from the independent variable (X) in the model. Higher R squared values indicate a better fit. Higher R2 values (closer to 1) indicate that a larger proportion of the variance in the dependent variable is explained by the model, suggesting a more accurate fit. The R – squared is defined as below:
R^2=1-(∑_(i=1)^n▒(Y_i-(Y_i ) ̂ )^2 )/(∑_(i=1)^n▒(Y_i-¯((Y_i ) ̂ ))^2 )

	Mean Absolute Error (MAE): MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. Similar to RMSE, a lower MAE indicates better predictive accuracy, measuring the average magnitude of errors without considering their direction.
MAE=(∑_(i=1)^n▒|Y_i-(Y_i ) ̂ |^2 )/n

	Mean Absolute Percentage Error (MAPE): MAPE expresses the average absolute percentage error between predicted and actual values relative to the actual values. It provides insight into the accuracy of predictions in terms of percentage. MAPE provides a measure of prediction accuracy in terms of relative error percentage. A lower MAPE suggests that the model's predictions are closer to the actual values. MAPE is defined below as:
MAPE=1/n ∑_1^n▒|Y_i-(Y_i ) ̂ |/Y_i 

	Technologies Used
The project leverages Python and its libraries for comprehensive data analysis and model implementation. Key libraries include Pandas for efficient data manipulation and analysis, Matplotlib and Seaborn for data visualization tasks. Scikit-learn is utilized extensively for implementing a variety of machine learning algorithms, facilitating model training, evaluation, and hyperparameter tuning. Additionally, XGBoost is employed specifically for gradient boosting models.
Results
Most of the EDA analysis replicating the visuals of [1] are presented in APPENDIX A. Figure 1 shows how hour of the day and day of the week determines the amount of appliance energy used. The heatmap shows peak energy consumption during specific hours on certain days, particularly in the evenings of Sunday, Tuesday, and Thursday, as well as mid-morning on Sunday. These
Figure 1: Appliance Energy Consumption with Day of the Week and Hour of the Day
 
peaks suggest that household activities requiring significant energy usage, such as cooking or laundry, are more concentrated during these periods.
The correlation heatmap presented in Figure 2 shows strong positive correlations between indoor temperatures and corresponding humidity levels within the same rooms. Additionally, variables such as Appliances and lights exhibit positive correlations with time-related features like Hour and NSM, suggesting that energy usage tends to peak during specific periods of the day. Additionally, indoor temperature variables in certain rooms, such as the kitchen (T1) and living room (T2), also show a positive correlation with Appliances, implying that as these rooms get warmer, there may be increased usage of appliances, possibly due to heating or cooling systems. Conversely, external weather factors, particularly RH_out (humidity outside), show a negative correlation with some indoor temperature variables.
Figure 2: Correlation Heatmap
 
Models Discussion
The evaluation of eight machine learning models—Multiple Linear Regression (LM), Support Vector Regression (SVR), Random Forest (RF), Decision Tree (DT), Gradient Boosting (GB), XGBoost (XGB), Extra Trees (ET), and Neural Network (NN)—are presented in Table 2. The performance metrics employed included Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and the coefficient of determination (R²).
The results obtained from this study are presented in Table 2 and compared to the original results in [1], [2] in Table 3. The results show some differences in the models’ performance which can be attributed to various factors such as data processing techniques, hyperparameter tuning, and the specific implementations of the machine learning models used. We compare the Multiple Linear Regression (LM), Support Vector Regression (SVR), Random Forest (RF) and the Gradient 
Table 2: Model performance – Training and Testing sets.
	Training RMSE	Training R2	Training MAE	Training MAPE %	Testing RMSE	Testing R2	Testing MAE	Testing MAPE %
LM	92.11	0.21	52.04	59.29	89.43	0.19	51.96	61.88
SVM	100.03	0.07	43.27	33.68	95.96	0.07	42.29	34.33
GBM	76.62	0.45	42.24	46.53	81.13	0.34	44.76	50.40
RF	25.26	0.94	11.98	11.75	65.11	0.57	31.21	31.42
XGB	17.66	0.97	11.65	15.74	65.18	0.57	33.71	36.04
ET	20.80	0.96	9.65	9.57	63.07	0.60	29.85	29.85
DT	62.10	0.64	22.52	19.57	82.82	0.31	35.54	31.13
NN	79.56	0.41	43.19	47.52	80.95	0.34	44.38	50.08

Table 3: Model performance from Original Study – Training and Testing sets.
	Training RMSE	Training R2	Training MAE	Training MAPE %	Testing RMSE	Testing R2	Testing MAE	Testing MAPE %
LM	93.21	0.18	53.13	61.32	93.18	0.16	51.97	59.93
SVM	39.35	0.85	15.08	15.6	70.74	0.52	31.36	29.76
GBM	17.56	0.97	11.97	16.27	66.65	0.57	35.22	38.29
RF	29.61	0.92	13.75	13.43	68.48	0.54	31.85	31.39
XGB**					63.86	0.61	30.24	29.78
ET**					59.61	0.66	26.62	25.37
** From [2]. Only presented testing performance.

Boosting (GB) models to [1]. Similarly, the results from XGBoost (XGB) and Extra Trees (ET) are compared to [2] and finally, the newly added models — Neural Network (NN) and Decision Tree (DT) — are compared to the individual results.
The linear regression model in the original studies showed a training RMSE of 93.21 and a testing RMSE of 93.18, with a corresponding training R² of 0.18 and testing R² of 0.16. This study found an improvement in these metrics, with a better training RMSE of 92.11 but a significantly improved testing RMSE of 89.43. The R² values also saw an improvement, with a training R² of 0.21 and a testing R² of 0.19. These improvements suggest that the modifications made, such as feature engineering or model optimization techniques, had a positive impact on the performance of the LM model. However, the slight variations in MAE and MAPE between the two results indicate that while the overall fit improved, the model's ability to minimize absolute errors and percentage errors still faces challenges, which could be due to inherent limitations in the linear assumptions of the model.
For the Support Vector Machine (SVM), there is a notable divergence between the original and the results of this study. The original model performed exceptionally well with a training RMSE of 39.35 and a testing RMSE of 70.74, along with a high training R² of 0.85. In contrast, the results indicated a much higher training RMSE of 100.03 and a testing RMSE of 95.96, with a significantly lower R² of 0.07. This stark difference could be attributed to several factors, such as differences in kernel selection, hyperparameter settings (like gamma and C values), or even the scale of the data. It appears that the SVM model struggled to generalize well, possibly due to overfitting during training or inadequate feature scaling, which often plays a crucial role in SVM performance. This result highlights the need for more refined hyperparameter tuning and possibly exploring different kernels to enhance model performance.
When comparing the Gradient Boosting Machine (GBM), the original results indicated strong performance with a training RMSE of 17.56 and a testing RMSE of 66.65. However, the GBM results obtained showed a significant decrease in performance, with a training RMSE of 76.62 and a testing RMSE of 81.13. The decrease in the R² values from 0.97 in the original to 0.45 in this work suggests that the model was not able to capture the complexity of the data as effectively. This could be due to differences in hyperparameter optimization, such as the number of boosting rounds, learning rate, or maximum tree depth. Additionally, it is possible that changes in feature engineering or data preprocessing steps in this work led to less informative features being used, thereby reducing the model's predictive accuracy.
For the Random Forest (RF) model, the original results showed a solid performance with a training RMSE of 29.61 and a testing RMSE of 68.48, with a training R² of 0.92. The results from this study, however, demonstrated a significantly improved performance with a lower training RMSE of 25.26 and a testing RMSE of 65.11, accompanied by a training R² of 0.94 and testing R² of 0.57. This improvement suggests that the adjustments made, possibly in the form of better hyperparameter tuning (like the number of trees or maximum features) or more effective feature selection, led to a more robust model that generalized better to the testing data. The reduced errors and higher R² values indicate that the RF model in this work was able to capture the underlying patterns in the data more effectively than in the original study.
In the case of the XGBoost (XGB) model, the available testing RMSE of 63.86 and testing R² of 0.61 suggest strong performance. The results from this study also showed a high level of performance with a training RMSE of 17.66 and a testing RMSE of 65.18, along with a training R² of 0.97 and a testing R² of 0.57. The slight increase in testing RMSE in these results compared to the original indicates that while the model was well-tuned during training, it may have experienced a slight overfitting or the data split may have had slightly different characteristics. Nonetheless, the high R² values across both results demonstrate the effectiveness of XGBoost as a powerful predictive model in this context. The Extra Trees (ET) model, similar to XGBoost, showed strong performance in both the original and this study. The original model had a testing RMSE of 59.61 and a testing R² of 0.66, while this study showed a comparable testing RMSE of 63.07 and a testing R² of 0.60. The close alignment between these results suggests that the Extra Trees model is highly consistent and robust across different implementations, likely due to its ability to minimize variance through ensemble learning. The slight differences could be attributed to variations in the number of trees or other hyperparameters, but overall, the ET model appears to be a reliable choice for this prediction task.
Finally, the Decision Tree (DT) and Neural Network (NN) models in this study did not perform as well as some of the other models. The DT model had a testing RMSE of 82.82 and a testing R² of 0.31, while the NN model showed a testing RMSE of 80.95 and a testing R² of 0.34. These results suggest that both models struggled to capture the complexity of the data, likely due to overfitting in the case of DT (which is sensitive to tree depth) and insufficient tuning or architecture selection in the case of NN.
The feature importance plot presented in Figure 3 reveals that temporal features play a dominant role in predicting appliance energy consumption in the Extra Trees model. The most important feature is NSM (Number of Seconds from Midnight), which suggests that the time of day is a critical factor in determining energy usage patterns. This is further supported by the high importance of transformed hour features (Hour_cos and Hour_sin), which capture cyclical daily patterns, and the direct hour of the day. These features indicate that the model relies heavily on the time-specific characteristics of energy consumption.
Figure 3: Feature Importance from the Extra Trees Model
 
In addition to temporal features, the energy usage of light fixtures (Lights) and room-specific conditions, such as temperature and humidity (e.g., T3, RH_1, RH_3), also play significant roles. These factors are likely linked to the operation of climate control systems and other appliances that respond to environmental conditions. The moderate importance of these features suggests that while the time of day drives the overall pattern of energy use, the specific conditions within the home also influence the intensity and timing of appliance usage.
Environmental conditions, such as wind speed and atmospheric pressure, have a noticeable but lesser impact, indicating that external weather factors do play a role in energy consumption. Features like Season and Week Status are found to be the least important. This suggests that broader temporal aggregates are less informative than specific time-of-day data.
Partial Dependence
The partial dependence plots presented in Figure 4 provide a detailed look into how each of the most significant features influences appliance energy consumption while holding other features constant. This allows us to interpret the unique effect of each variable on appliance energy use. The plot for the Number of Seconds from Midnight (NSM) shows a generally upward trend, indicating that energy consumption tends to increase as the day progresses. This aligns with typical daily activities where energy use rises from morning through the day, likely peaking during evening hours when households are most active. The plot captures this daily trend, suggesting that time-of-day is a critical determinant of energy consumption patterns.
Figure 4: Partial dependence of top 6 features
 
The Hour plot shows different peaks in energy consumption during certain hours, particularly in the morning and evening, which corresponds with common patterns of household energy use. The `Lights` feature, on the other hand, shows a strong positive relationship with overall energy consumption. As the use of lighting increases, so does the total energy consumption, which is expected given that lighting is a major contributor to household energy use, especially during the evening when lights are most frequently used. These plots provide a comprehensive view of how time-related features and specific appliance usage drive energy consumption, offering valuable insights for managing energy use more efficiently.
Residual Distribution
The residual plot in Figure 5 gives a visual representation of how frequently different levels of error occur across all predictions. A well-calibrated model will produce a residual distribution that is centered around zero, with most residuals clustered close to zero, indicating that the model's predictions are generally accurate. In the histogram provided, the residuals are indeed centered around zero, forming a sharp peak, which suggests that the majority of the model's predictions are
Figure 5: Residuals Distribution
 
very close to the actual values. This sharp peak is a positive indicator, showing that the model has low bias. However, the presence of long tails on either side of the distribution indicates that there are some predictions with larger errors. These tails are areas of potential concern as they suggest that while the model performs well for most predictions, there are specific instances where it does not perform as accurately. Investigating these instances could uncover specific conditions or data characteristics that challenge the model, offering opportunities for further refinement.
Residual Plots
While Figure 5 shows the overall distribution of the residuals, the residual plots in Figure 6 display the distribution of the residuals with respect to the values of the NSM. In this context, the residual plot against the `NSM` feature (Number of Seconds from Midnight) is particularly important because NSM is the dominant contributing factor to appliance early prediction as indicated earlier. 
Figure 6: How the Residuals change with the NSM
 
Ideally, residuals should be randomly scattered around the horizontal axis (at zero), indicating that the model has no systematic errors and that its predictions are unbiased across all ranges of the feature. This is the case in the residual plot in Figure 6. The points are spread out horizontally across different values of `NSM`, with no clear pattern or trend. This randomness suggests that the model does not deliberately overpredict or underpredict energy consumption based on the time of day. However, there are some larger residuals (both positive and negative), which indicate instances where the model's predictions are off by a significant margin. These outliers could be due to specific conditions or anomalies in the data that the model fails to capture.
Cross-Validation RMSE Distribution
Figure 7: Distribution of Cross Validation Scores
 
The cross-validation RMSE distribution in Figure 7 is vital for evaluating the stability and reliability of the model across different subsets of the data. Cross-validation is a robust method to assess how well the model generalizes to independent data sets. The approach splits the data into different folds and evaluate the model's performance on each fold. The histogram of cross-validation RMSE scores shows how these errors are distributed across different folds of the data. In the graph, the RMSE values appear to be concentrated around a central peak with values of 68 - 70, indicating that the model's performance is consistent across different data splits. This consistency suggests that the model is not overfitting to specific parts of the data and is likely to perform well on unseen data. However, the spread in the RMSE values—reflected in the width of the histogram—indicates variability in performance, which could be due to the model's sensitivity to specific data characteristics.
Figure 8: Bootstrapped RMSE Distribution
 
A similar visualization to the cross validation RMSE errors in Figure 7 is the Bootstrapped RMSE distribution in Figure 8. Bootstrapping involves repeatedly sampling from the dataset with replacement to create multiple training sets, from which the model is then trained and evaluated. This process generates a distribution of RMSE values, providing an estimate of how the model's performance might vary under different data conditions.
In the bootstrapped RMSE distribution, the histogram shows the spread of RMSE values across these bootstrapped samples. The x-axis represents the RMSE values, ranging from approximately 58 to 68, while the y-axis represents the frequency, or how many times each range of RMSE values occurred across the bootstrap samples. The histogram shows that the majority of RMSE values are clustered around the center, particularly between 60 and 64. This clustering indicates that the model's prediction errors are relatively consistent across different bootstrap samples, with most errors falling within this central range. The peak of the histogram, around an RMSE of 62, suggests that this is the most common error rate across the different samples, reflecting the model's typical performance. Additionally, the distribution appears to be roughly normal but with a slight skew to the right, meaning that while most bootstrapped RMSE values are concentrated around the central value, there are some instances where the model yields higher errors, resulting in a longer tail on the right side of the distribution. The Kernel Density Estimate (KDE) curve, drawn on the histogram provides a smooth approximation of this distribution, further highlighting the central tendency and the spread of errors. 
Conclusion
The overall study aimed to improve the prediction of appliance energy consumption in residential buildings by experimenting with various machine learning models and implementing thoughtful feature engineering. The study expanded on previous work by incorporating Decision Tree (DT) and Neural Network (NN) models, alongside traditional and ensemble methods like Linear Regression, Support Vector Machines (SVM), Gradient Boosting Machines (GBM), Random Forest (RF), XGBoost (XGB), and Extra Trees (ET). The added models provided a broader perspective on the types of algorithms that could be effective in this context, each contributing unique strengths and weaknesses to the overall model comparison. The Decision Tree model, when carefully tuned for maximum depth and other critical hyperparameters, demonstrated solid performance, particularly in balancing complexity and generalization. Although it did not outperform the ensemble methods like Extra Trees and XGBoost, it provided a straightforward and interpretable approach that effectively captured the underlying patterns in the data. The Neural Network model, on the other hand, required extensive hyperparameter tuning to optimize its architecture, including the number of hidden layers, activation function, and learning rate. While the Neural Network showed promise, particularly in capturing non-linear relationships, it struggled to outperform the tree-based models, indicating that the latter might be better suited for the specific characteristics of the appliance energy consumption dataset. Feature engineering played a pivotal role in enhancing model performance across the board. The inclusion of time-related features, such as the Number of Seconds from Midnight (NSM) and transformed hour features (Hour_cos and Hour_sin), proved to be highly influential in improving the accuracy of predictions. These features captured the cyclical nature of energy use throughout the day and contributed significantly to the model’s predictive power. Moreover, the environmental and room-specific variables provided additional context, allowing the models to better understand the conditions under which energy consumption varied. The success of these features was particularly evident in the performance of the ensemble models like Extra Trees and XGBoost, which effectively leveraged the rich feature set to deliver the most accurate predictions.
Reference
[1]	L. Candanedo, V. Feldheim, and D. Deramaix, “Data driven prediction models of energy use of appliances in a low-energy house,” Energy Build, vol. 140, pp. 81–97, 2017, doi: 10.1016/J.ENBUILD.2017.01.083.
[2]	C. F. Assadian and F. Assadian, “Data-Driven Modeling of Appliance Energy Usage,” Energies (Basel), p., 2023, doi: 10.3390/en16227536.
[3]	Y. Xie and A. I. M. Noor, “Factors Affecting Residential End-Use Energy: Multiple Regression Analysis Based on Buildings, Households, Lifestyles, and Equipment,” Buildings, p., 2022, doi: 10.3390/buildings12050538.
[4]	P. Rickwood, “Residential operational energy use,” Urban Policy and Research, vol. 27, no. 2, 2009, doi: 10.1080/08111140902950495.
[5]	A. Kavousian, R. Rajagopal, and M. Fischer, “Determinants of residential electricity consumption: Using smart meter data to examine the effect of climate, building characteristics, appliance stock, and occupants’ behavior,” Energy, vol. 55, pp. 184–194, 2013, doi: 10.1016/J.ENERGY.2013.03.086.
[6]	E. Leahy and S. Lyons, “Energy use and appliance ownership in Ireland,” Energy Policy, vol. 38, pp. 4265–4279, 2010, doi: 10.1016/J.ENPOL.2010.03.056.
[7]	K. S. Cetin, P. Tabares-Velasco, and A. Novoselac, “Appliance daily energy use in new residential buildings: Use profiles and variation in time-of-use,” Energy Build, vol. 84, pp. 716–726, 2014, doi: 10.1016/J.ENBUILD.2014.07.045.
[8]	A. Kavousian, R. Rajagopal, and M. Fischer, “Ranking appliance energy efficiency in households: Utilizing smart meter data and energy efficiency frontiers to estimate and identify the determinants of appliance energy efficiency in residential buildings,” Energy Build, vol. 99, 2015, doi: 10.1016/j.enbuild.2015.03.052.
[9]	J. Rouleau, L. Gosselin, and P. Blanchet, “Understanding energy consumption in high-performance social housing buildings : a case study from Canada,” Energy, vol. 145, pp. 677–690, 2018, doi: 10.1016/J.ENERGY.2017.12.107.
[10]	A. Iwayemi, W. Wan, and C. Zhou, “Energy management for intelligent buildings,” Energy Manag. Syst, 2011.
[11]	J. Reyna and M. Chester, “Energy efficiency to reduce residential electricity and natural gas use under climate change,” Nat Commun, vol. 8, p., 2017, doi: 10.1038/ncomms14916.
[12]	S. Ullah, M. Nazeer, and N. Malik, “Machine Learning based Energy Consumption Prediction of Appliances in a Low Energy House,” International journal of Engineering Works, p., 2020, doi: 10.34259/IJEW.20.710326332.
[13]	G. R. Duarte, L. G. da Fonseca, P. V. Z. C. Goliatt, and A. C. de C. Lemonge, “Comparison of machine learning techniques for predicting energy loads in buildings,” Ambiente Construído, vol. 17, no. 3, 2017.
[14]	M. Rambabu, N. Ramakrishna, and P. K. Polamarasetty, “Prediction and Analysis of Household Energy Consumption by Machine Learning Algorithms in Energy Management,” E3S Web of Conferences, p., 2022, doi: 10.1051/e3sconf/202235002002.
[15]	R. E. Edwards, J. New, and L. E. Parker, “Predicting future hourly residential electrical consumption: A machine learning case study,” Energy Build, vol. 49, pp. 591–603, 2012, doi: 10.1016/J.ENBUILD.2012.03.010.
[16]	R. K. Jain, K. Smith, P. Culligan, and J. Taylor, “Forecasting energy consumption of multi-family residential buildings using support vector regression: Investigating the impact of temporal and spatial monitoring granularity on performance accuracy,” Appl Energy, vol. 123, pp. 168–178, 2014, doi: 10.1016/J.APENERGY.2014.02.057.
[17]	L. Xiang, T. Xie, and W. Xie, “Prediction model of household appliance energy consumption based on machine learning,” J Phys Conf Ser, vol. 1453, p., 2020, doi: 10.1088/1742-6596/1453/1/012064.
[18]	E. Mocanu, H. Nguyen, M. Gibescu, and W. Kling, “Deep learning for estimating building energy consumption,” Sustainable Energy, Grids and Networks, vol. 6, pp. 91–99, 2016, doi: 10.1016/J.SEGAN.2016.02.005.
[19]	J. Jiang, Q. Kong, M. D. Plumbley, and N. Gilbert, “Deep Learning Based Energy Disaggregation and On/Off Detection of Household Appliances,” ArXiv, vol. abs/1908.00941, p., 2019, doi: 10.1145/3441300.
[20]	S. Bourhnane, M. Abid, R. Lghoul, K. Zine-dine, N. Elkamoun, and D. Benhaddou, “Machine learning for energy consumption prediction and scheduling in smart buildings,” SN Appl Sci, vol. 2, pp. 1–10, 2020, doi: 10.1007/s42452-020-2024-9.
[21]	N.-T. Ngo, A. Pham, T. T. H. Truong, N.-S. Truong, N.-T. Huynh, and T. M. Pham, “An Ensemble Machine Learning Model for Enhancing the Prediction Accuracy of Energy Consumption in Buildings,” Arab J Sci Eng, vol. 47, pp. 4105–4117, 2021, doi: 10.1007/s13369-021-05927-7.
[22]	M. Al-Rakhami, A. Gumaei, A. Alsanad, A. Alamri, and M. M. Hassan, “An Ensemble Learning Approach for Accurate Energy Load Prediction in Residential Buildings,” IEEE Access, vol. 7, 2019, doi: 10.1109/ACCESS.2019.2909470.
[23]	L. N. M, K. C. K. S, A. Mohan, and V. Gopal, “Appliance Prediction from Total Energy Data — A Demand Response Method Using Simple and Complex Networks,” 2019 IEEE 2nd International Conference on Power and Energy Applications (ICPEA), pp. 222–226, 2019, doi: 10.1109/ICPEA.2019.8818489.
[24]	M. Sajjad et al., “A Novel CNN-GRU-Based Hybrid Approach for Short-Term Residential Load Forecasting,” IEEE Access, vol. 8, pp. 143759–143768, 2020, doi: 10.1109/access.2020.3009537.
[25]	D. M. Ibrahim, A. Almhafdy, A. A. Al-Shargabi, M. Alghieth, A. Elragi, and F. Chiclana, “The use of statistical and machine learning tools to accurately quantify the energy performance of residential buildings,” PeerJ Comput Sci, vol. 8, p., 2022, doi: 10.7717/peerj-cs.856.
[26]	A. Moradzadeh, B. Mohammadi-ivatloo, M. Abapour, A. Anvari‐Moghaddam, and S. S. Roy, “Heating and Cooling Loads Forecasting for Residential Buildings Based on Hybrid Machine Learning Applications: A Comprehensive Review and Comparative Analysis,” IEEE Access, vol. PP, p. 1, 2021, doi: 10.1109/access.2021.3136091.
[27]	Z. Wu and H. He, “Traditional Machine Learning Models for Building Energy Performance Prediction: A Comparative Research,” Machine Learning Research, vol. 8, no. 1, pp. 1–8, 2023.
[28]	N. S. I. M. Husin, S. Mostafa, M. M. Jaber, S. Gunasekaran, A. Al-shakarchi, and N. F. Abdulsattar, “Machine Learning Regression Approach for Estimating Energy Consumption of Appliances in Smart Home,” 2023 Al-Sadiq International Conference on Communication and Information Technology (AICCIT), pp. 229–233, 2023, doi: 10.1109/AICCIT57614.2023.10217991.
[29]	D.-H. Tran, D.-L. Luong, and J.-S. Chou, “Nature-inspired metaheuristic ensemble model for forecasting energy consumption in residential buildings,” Energy, vol. 191, p. 116552, 2020, doi: 10.1016/j.energy.2019.116552.
[30]	E. U. Haq, X. Lyu, Y. Jia, M. Hua, and F. Ahmad, “Forecasting household electric appliances consumption and peak demand based on hybrid machine learning approach,” Energy Reports, vol. 6, pp. 1099–1105, 2020, doi: 10.1016/j.egyr.2020.11.071.
[31]	H. G. Zaini, “Forecasting of Appliances House in a Low-Energy Depend on Grey Wolf Optimizer,” Computers, Materials & Continua, p., 2022, doi: 10.32604/cmc.2022.021998.
[32]	A. Al-Adaileh and S. Khaddaj, “Machine Learning Prediction Based Integrated Smart Energy Management System to Improve Home Energy Efficiency,” 2022 21st International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES), pp. 1–4, 2022, doi: 10.1109/DCABES57229.2022.00042.
[33]	S. Iram et al., “An Innovative Machine Learning Technique for the Prediction of Weather Based Smart Home Energy Consumption,” IEEE Access, vol. 11, pp. 76300–76320, 2023, doi: 10.1109/ACCESS.2023.3287145.
[34]	 and C.-J. L. Chih-Wei Hsu, Chih-Chung Chang, “A Practical Guide to Support Vector Classification,” BJU Int, vol. 101, no. 1, 2008.
[35]	S. Seyedzadeh, F. Rahimian, P. Rastogi, and I. Glesk, “Tuning machine learning models for prediction of building energy loads,” Sustain Cities Soc, p., 2019, doi: 10.1016/J.SCS.2019.101484.
[36]	J. Bergstra and Y. Bengio, “Random search for hyper-parameter optimization,” Journal of Machine Learning Research, vol. 13, 2012.
[37]	M. Feurer, A. Klein, K. Eggensperger, J. T. Springenberg, M. Blum, and F. Hutter, “Efficient and robust automated machine learning,” in Advances in Neural Information Processing Systems, 2015.
[38]	T. A. Ryttov and R. Shrock, “Infrared fixed point physics in so (Nc) and Sp (Nc) gauge theories,” Physical Review D, vol. 96, no. 10, 2017, doi: 10.1103/PhysRevD.96.105015.
[39]	S. R. Young, D. C. Rose, T. P. Karnowski, S. H. Lim, and R. M. Patton, “Optimizing deep learning hyper-parameters through an evolutionary algorithm,” in Proceedings of MLHPC 2015: Machine Learning in High-Performance Computing Environments - Held in conjunction with SC 2015: The International Conference for High Performance Computing, Networking, Storage and Analysis, 2015. doi: 10.1145/2834892.2834896.
[40]	D. Moldovan and A. Słowik, “Energy consumption prediction of appliances using machine learning and multi-objective binary grey wolf optimization for feature selection,” Appl. Soft Comput., vol. 111, p. 107745, 2021, doi: 10.1016/J.ASOC.2021.107745.
[41]	Y.-W. Chen and C.-J. Lin, “Combining SVMs with Various Feature Selection Strategies,” in Feature Extraction: Foundations and Applications, I. Guyon, M. Nikravesh, S. Gunn, and L. A. Zadeh, Eds., Berlin, Heidelberg: Springer Berlin Heidelberg, 2006, pp. 315–324. doi: 10.1007/978-3-540-35488-8_13.
[42]	I. T. Jolliffe, Principal component analysis for special types of data. Springer, 2002.
[43]	H. Abdi and L. J. Williams, “Principal component analysis,” 2010. doi: 10.1002/wics.101.
[44]	L. Candanedo, “Appliances Energy Prediction,” 2017.
[45]	S. Yadav and S. Shukla, “Analysis of k-Fold Cross-Validation over Hold-Out Validation on Colossal Datasets for Quality Classification,” in Proceedings - 6th International Advanced Computing Conference, IACC 2016, 2016. doi: 10.1109/IACC.2016.25.
 
Appendix A
      
